{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install polars\n\nprint(\"start\")\n\nimport time\nimport pandas as pd\nimport polars as pl\nimport numpy as np\nfrom collections import defaultdict\nfrom sklearn.preprocessing import label_binarize\n\nfrom gensim.test.utils import common_texts\nfrom gensim.models import Word2Vec\n\ntrain = pl.read_parquet('/kaggle/input/otto-train-and-test-data-for-local-validation/test.parquet')\ntest = pl.read_parquet('/kaggle/input/otto-full-optimized-memory-footprint/test.parquet')\n\n# train = pl.read_parquet('/kaggle/input/small-data/train.parquet')\n# test = pl.read_parquet('/kaggle/input/small-data/test.parquet')\n\ntrain = train.with_columns([\n    pl.col('session').cast(pl.datatypes.Int32),\n    pl.col('type').cast(pl.datatypes.UInt8),\n    pl.col('aid').cast(pl.datatypes.Int32),\n    pl.col('ts').cast(pl.datatypes.Int64)\n])\n\ntest = test.with_columns([\n    pl.col('session').cast(pl.datatypes.Int32),\n    pl.col('type').cast(pl.datatypes.UInt8),\n    pl.col('aid').cast(pl.datatypes.Int32),\n    pl.col('ts').cast(pl.datatypes.Int64)\n])\n\n##### for Word2Vec pretrain\nsentences_df = pl.concat([train, test]).groupby('session').agg(\n    pl.col('aid').alias('sentence')\n)\n\nsentences = sentences_df['sentence'].to_list()\n\nprint(\"w2vec start\")\nstart = time.time()\n\nw2vec = Word2Vec(sentences=sentences, vector_size=32, min_count=1, workers=4)\n\nprint(\"w2vec end\")\nend = time.time()\nprint(f'執行時間: {end - start} 秒\\n')\n\nfrom annoy import AnnoyIndex\n\naid2idx = {aid: i for i, aid in enumerate(w2vec.wv.index_to_key)}\nindex = AnnoyIndex(32, 'euclidean')\n\nfor aid, idx in aid2idx.items():\n    index.add_item(idx, w2vec.wv.vectors[idx])\n    \nindex.build(10)\n#####\n\n###\ntrain_labels = pl.read_parquet('/kaggle/input/otto-train-and-test-data-for-local-validation/test_labels.parquet')\n\ndef word2vec_candidate(df): \n    global index\n    session_types = ['clicks', 'carts', 'orders']\n    df_session_AIDs = df.to_pandas().reset_index(drop=True).groupby('session')['aid'].apply(list)\n    df_session_types = df.to_pandas().reset_index(drop=True).groupby('session')['type'].apply(list)\n    df_session_num = df.to_pandas().reset_index(drop=True).groupby('session')['session'].apply(list)\n    \n    #\n    label_sessions = []\n    label_aids = []\n\n    print(\"candidate calc start\")\n    start = time.time()\n\n    type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n    for AIDs, types, session_num in zip(df_session_AIDs, df_session_types, df_session_num):\n        session_num = session_num[0]\n            \n        if len(AIDs) >= 20:\n            # if we have enough aids (over equals 20) we don't need to look for candidates! we just use the old logic\n            weights=np.logspace(0.1,1,len(AIDs),base=2, endpoint=True)-1\n            aids_temp=defaultdict(lambda: 0)\n            for aid,w,t in zip(AIDs,weights,types): \n                aids_temp[aid]+= w * type_weight_multipliers[t]\n                \n            sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n\n            if len(sorted_aids) < 20:\n                AIDs = list(dict.fromkeys(AIDs[::-1]))\n                # let's grab the most recent aid\n                most_recent_aid = AIDs[0]\n                # and look for some neighbors!\n                nns = [w2vec.wv.index_to_key[i] for i in index.get_nns_by_item(aid2idx[most_recent_aid], 21)[1:]]\n                sorted_aids = (sorted_aids + nns)\n           \n            session_arr = [session_num for i in range(20)]\n            # \n            label_sessions.extend(session_arr)\n            label_aids.extend(sorted_aids[:20])\n\n        else:\n            # here we don't have 20 aids to output -- we will use word2vec embeddings to generate candidates!\n            AIDs = list(dict.fromkeys(AIDs[::-1]))\n            # let's grab the most recent aid\n            most_recent_aid = AIDs[0]\n            # and look for some neighbors!\n            nns = [w2vec.wv.index_to_key[i] for i in index.get_nns_by_item(aid2idx[most_recent_aid], 21)[1:]]\n            \n            session_arr = [session_num for i in range(20)]\n            label_sessions.extend(session_arr)\n            label_aids.extend((AIDs+nns)[:20])  \n\n    candidates = pl.DataFrame({\"session\": label_sessions, \"aid\":label_aids})\n    candidates = candidates.with_columns([\n        pl.col('session').cast(pl.datatypes.Int32),\n        pl.col('aid').cast(pl.datatypes.Int32),\n    ])\n    \n    print('candidates')\n    print(candidates)\n    \n    print(\"candidate calc end\")\n    end = time.time()\n    print(f'執行時間: {end - start} 秒\\n')\n\n    \n    candidates = candidates.with_column(pl.col('aid').cumcount().over('session').alias('word2vec_rank') + 1)\n    candidates = candidates.with_columns([\n        pl.col('session').cast(pl.datatypes.Int32),\n    ])\n    \n    df = df.join(candidates, on=['session', 'aid'], how='outer').sort(\"session\")\n    return df\n\ndef add_action_num_reverse_chrono(df):\n    return df.select([\n        pl.col('*'),\n        pl.col('session').cumcount().reverse().over('session').alias('action_num_reverse_chrono')\n    ])\n\ndef add_session_length(df):\n    return df.select([\n        pl.col('*'),\n        pl.col('session').count().over('session').alias('session_length')\n    ])\n\ndef add_log_recency_score(df):\n    linear_interpolation = 0.1 + ((1-0.1) / (df['session_length']-1)) * (df['session_length']-df['action_num_reverse_chrono']-1)\n    return df.with_columns(pl.Series(2**linear_interpolation - 1).alias('log_recency_score')).fill_nan(1)\n\ndef add_type_weighted_log_recency_score(df):\n    type_weights = {0:1, 1:6, 2:3}\n    type_weighted_log_recency_score = pl.Series(df['log_recency_score'] / df['type'].apply(lambda x: type_weights[x]))\n    return df.with_column(type_weighted_log_recency_score.alias('type_weighted_log_recency_score'))\n\ndef apply(df, pipeline):\n    for f in pipeline:\n        df = f(df)\n    return df\n\npipeline = [add_action_num_reverse_chrono, add_session_length, add_log_recency_score, add_type_weighted_log_recency_score, word2vec_candidate]\n\ntrain = apply(train, pipeline)\n\ntype2id = {\"clicks\": 0, \"carts\": 1, \"orders\": 2}\n\ntrain_labels = train_labels.explode('ground_truth').with_columns([\n    pl.col('ground_truth').alias('aid'),\n    pl.col('type').apply(lambda x: type2id[x])\n])[['session', 'type', 'aid']]\n\ntrain_labels = train_labels.with_columns([\n    pl.col('session').cast(pl.datatypes.Int32),\n    pl.col('type').cast(pl.datatypes.UInt8),\n    pl.col('aid').cast(pl.datatypes.Int32)\n])\n\ntrain_labels = train_labels.with_column(pl.lit(1).alias('gt'))\n\ntrain = train.join(train_labels, how='left', on=['session', 'type', 'aid']).with_column(pl.col('gt').fill_null(0))\n\ndef get_session_lenghts(df):\n    return df.groupby('session').agg([\n        pl.col('session').count().alias('session_length')\n    ])['session_length'].to_numpy()\n\nsession_lengths_train = get_session_lenghts(train)\n\nfrom lightgbm.sklearn import LGBMRanker\n\nranker = LGBMRanker(\n    objective=\"lambdarank\",\n    metric=\"ndcg\",\n    boosting_type=\"dart\",\n    n_estimators=20,\n    importance_type='gain',\n)\n\nfeature_cols = ['aid', 'type', 'action_num_reverse_chrono', 'session_length', 'log_recency_score', 'type_weighted_log_recency_score', 'word2vec_rank']\ntarget = 'gt'\n\nranker = ranker.fit(\n    train[feature_cols].to_pandas(),\n    train[target].to_pandas(),\n    group=session_lengths_train,\n)\n\n### create submission\ntest = apply(test, pipeline)\n\nscores = ranker.predict(test[feature_cols].to_pandas())\n\ntest = test.with_columns(pl.Series(name='score', values=scores))\ntest_predictions = test.sort(['session', 'score'], reverse=True).groupby('session').agg([\n    pl.col('aid').limit(20).list()\n])\n\nsession_types = []\nlabels = []\n\nfor session, preds in zip(test_predictions['session'].to_numpy(), test_predictions['aid'].to_numpy()):\n    l = ' '.join(str(p) for p in preds)\n    for session_type in ['clicks', 'carts', 'orders']:\n        labels.append(l)\n        session_types.append(f'{session}_{session_type}')\n\nsubmission = pl.DataFrame({'session_type': session_types, 'labels': labels})\nsubmission.write_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-01T06:54:28.601370Z","iopub.execute_input":"2023-01-01T06:54:28.601892Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: polars in /opt/conda/lib/python3.7/site-packages (0.15.9)\nRequirement already satisfied: typing_extensions>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from polars) (4.1.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mstart\nw2vec start\n","output_type":"stream"}]}]}