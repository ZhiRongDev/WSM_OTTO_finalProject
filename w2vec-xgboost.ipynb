{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this notebook we will train an `XGBoost Ranker` on the GPU and perform prediction.\n\nTraining with varied architectures and ensembling (please see [💡 [2 methods] How-to ensemble predictions 🏅🏅🏅](https://www.kaggle.com/code/radek1/2-methods-how-to-ensemble-predictions) for a tutorial on ensembling) can offer you a significant jump on the LB!\n\nTraining with `XGBoost` however offers more additional advantages. In comparison to `LGBM`, `XGBoost` allows you to train with the following objectives (`LGBM` gives you access to a single loss only for ranking, training with different objectives is a great way of improving your ensemble!):\n* `rank:pairwise`\n* `rank:ndcg`\n* `rank:map`\n\nOn top of that, we will train on the GPU! 🔥 GPU can offer a significant speed-up. You can train more and bigger models in a shorter amount of time. However, when training on the GPU with large amounts of tabular data, you can easily run into problems (how to load the data onto the GPU for processing in chunks, how to manage memory).\n\nAs we want to focus on feature engineering and training lets offload all the low level, tedious considerations to the `Merlin Framework`!\n\nIn this notebook, we will introduce the entire pipeline. We will preprocess our data on the GPU using a library specifically designed for tabular data preprocessing, `NVTabular`. We will then proceed to train our `XGBoost` model with `Merlin Models`. In the background  we will leverage `dask_cuda` and distributed training to optimize the use of available GPU RAM, but we will let the libraries handle all that! No additional configuration will be required from us.\n\nLet's get started!\n\n## Other resources you might find useful:\n\n* [💡 [2 methods] How-to ensemble predictions 🏅🏅🏅](https://www.kaggle.com/code/radek1/2-methods-how-to-ensemble-predictions)\n* [co-visitation matrix - simplified, imprvd logic 🔥](https://www.kaggle.com/code/radek1/co-visitation-matrix-simplified-imprvd-logic)\n* [💡 Word2Vec How-to [training and submission]🚀🚀🚀](https://www.kaggle.com/code/radek1/word2vec-how-to-training-and-submission)\n* [local validation tracks public LB perfecty -- here is the setup](https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991)\n* [💡 For my friends from Twitter and LinkedIn -- here is how to dive into this competition 🐳](https://www.kaggle.com/competitions/otto-recommender-system/discussion/368560)\n* [Full dataset processed to CSV/parquet files with optimized memory footprint](https://www.kaggle.com/competitions/otto-recommender-system/discussion/363843)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Libraries installation\n\nWe will need a couple of libraries that do not come preinstalled on the Kaggle VM. Let's install them here.","metadata":{}},{"cell_type":"code","source":"!pip install polars\n!pip install nvtabular==1.3.3 merlin-models polars merlin-core==v0.4.0 dask_cuda","metadata":{"execution":{"iopub.status.busy":"2023-01-03T07:38:44.197118Z","iopub.execute_input":"2023-01-03T07:38:44.198392Z","iopub.status.idle":"2023-01-03T07:38:44.202254Z","shell.execute_reply.started":"2023-01-03T07:38:44.198351Z","shell.execute_reply":"2023-01-03T07:38:44.201139Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Data Processing","metadata":{}},{"cell_type":"markdown","source":"We will briefly preprocess our data using polars. After that step, we will hand it over to `NVTabular` to tag our data (so that our model will know where to find the information it needs for training).","metadata":{}},{"cell_type":"code","source":"from nvtabular import *\nfrom merlin.schema.tags import Tags\nimport polars as pl\nimport xgboost as xgb\n\nfrom merlin.core.utils import Distributed\nfrom merlin.models.xgb import XGBoost\nfrom nvtabular.ops import AddTags","metadata":{"execution":{"iopub.status.busy":"2023-01-03T07:38:44.204501Z","iopub.execute_input":"2023-01-03T07:38:44.205360Z","iopub.status.idle":"2023-01-03T07:38:44.214965Z","shell.execute_reply.started":"2023-01-03T07:38:44.205327Z","shell.execute_reply":"2023-01-03T07:38:44.213821Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"print(\"start\")\n\nimport time\nimport gc\nimport pandas as pd\nimport polars as pl\nimport numpy as np\nfrom collections import defaultdict\n# import cudf\nfrom sklearn.preprocessing import label_binarize\n\nfrom gensim.test.utils import common_texts\nfrom gensim.models import Word2Vec\n\nall_train = pl.read_parquet('/kaggle/input/otto-full-optimized-memory-footprint/train.parquet')\ntest = pl.read_parquet('/kaggle/input/otto-full-optimized-memory-footprint/test.parquet')\n\nall_train = all_train.with_columns([\n    pl.col('session').cast(pl.datatypes.Int32),\n    pl.col('type').cast(pl.datatypes.UInt8),\n    pl.col('aid').cast(pl.datatypes.Int32),\n    pl.col('ts').cast(pl.datatypes.Int64)\n])\n\ntest = test.with_columns([\n    pl.col('session').cast(pl.datatypes.Int32),\n    pl.col('type').cast(pl.datatypes.UInt8),\n    pl.col('aid').cast(pl.datatypes.Int32),\n    pl.col('ts').cast(pl.datatypes.Int64)\n])\n\n##### for Word2Vec pretrain\nsentences_df = pl.concat([all_train, test]).groupby('session').agg(\n    pl.col('aid').alias('sentence')\n)\n\nsentences = sentences_df['sentence'].to_list()\n\ndel all_train\ngc.collect()\n\ntrain = pl.read_parquet('/kaggle/input/otto-train-and-test-data-for-local-validation/test.parquet')\n\ntrain = train.with_columns([\n    pl.col('session').cast(pl.datatypes.Int32),\n    pl.col('type').cast(pl.datatypes.UInt8),\n    pl.col('aid').cast(pl.datatypes.Int32),\n    pl.col('ts').cast(pl.datatypes.Int64)\n])\n\nprint(\"w2vec start\")\nstart = time.time()\n\nw2vec = Word2Vec(sentences=sentences, vector_size=32, min_count=1, workers=4)\n\nprint(\"w2vec end\")\nend = time.time()\nprint(f'執行時間: {end - start} 秒\\n')\n\nfrom annoy import AnnoyIndex\n\naid2idx = {aid: i for i, aid in enumerate(w2vec.wv.index_to_key)}\nindex = AnnoyIndex(32, 'euclidean')\n\nfor aid, idx in aid2idx.items():\n    index.add_item(idx, w2vec.wv.vectors[idx])\n    \nindex.build(10)\n#####\n\n###\ntrain_labels = pl.read_parquet('/kaggle/input/otto-train-and-test-data-for-local-validation/test_labels.parquet')\n\ndef word2vec_candidate(df): \n    global index\n    session_types = ['clicks', 'carts', 'orders']\n    df_session_AIDs = df.to_pandas().reset_index(drop=True).groupby('session')['aid'].apply(list)\n    df_session_types = df.to_pandas().reset_index(drop=True).groupby('session')['type'].apply(list)\n    df_session_num = df.to_pandas().reset_index(drop=True).groupby('session')['session'].apply(list)\n    \n    #\n    label_sessions = []\n    label_aids = []\n\n    print(\"candidate calc start\")\n    start = time.time()\n\n    type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n    for AIDs, types, session_num in zip(df_session_AIDs, df_session_types, df_session_num):\n        session_num = session_num[0]\n            \n        if len(AIDs) >= 20:\n            # if we have enough aids (over equals 20) we don't need to look for candidates! we just use the old logic\n            weights=np.logspace(0.1,1,len(AIDs),base=2, endpoint=True)-1\n            aids_temp=defaultdict(lambda: 0)\n            for aid,w,t in zip(AIDs,weights,types): \n                aids_temp[aid]+= w * type_weight_multipliers[t]\n                \n            sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n\n            if len(sorted_aids) < 20:\n                AIDs = list(dict.fromkeys(AIDs[::-1]))\n                # let's grab the most recent aid\n                most_recent_aid = AIDs[0]\n                # and look for some neighbors!\n                nns = [w2vec.wv.index_to_key[i] for i in index.get_nns_by_item(aid2idx[most_recent_aid], 21)[1:]]\n                sorted_aids = (sorted_aids + nns)\n           \n            session_arr = [session_num for i in range(20)]\n            # \n            label_sessions.extend(session_arr)\n            label_aids.extend(sorted_aids[:20])\n\n        else:\n            # here we don't have 20 aids to output -- we will use word2vec embeddings to generate candidates!\n            AIDs = list(dict.fromkeys(AIDs[::-1]))\n            # let's grab the most recent aid\n            most_recent_aid = AIDs[0]\n            # and look for some neighbors!\n            nns = [w2vec.wv.index_to_key[i] for i in index.get_nns_by_item(aid2idx[most_recent_aid], 21)[1:]]\n            \n            session_arr = [session_num for i in range(20)]\n            label_sessions.extend(session_arr)\n            label_aids.extend((AIDs+nns)[:20])  \n\n    candidates = pl.DataFrame({\"session\": label_sessions, \"aid\":label_aids})\n    candidates = candidates.with_columns([\n        pl.col('session').cast(pl.datatypes.Int32),\n        pl.col('aid').cast(pl.datatypes.Int32),\n    ])\n    \n    print('candidates')\n    print(candidates)\n    \n    print(\"candidate calc end\")\n    end = time.time()\n    print(f'執行時間: {end - start} 秒\\n')\n    \n    candidates = candidates.with_column(pl.col('aid').cumcount().over('session').alias('word2vec_rank') + 1)\n    candidates = candidates.with_columns([\n        pl.col('session').cast(pl.datatypes.Int32),\n    ])\n    \n    df = df.join(candidates, on=['session', 'aid'], how='outer').sort(\"session\")\n    return df\n\ndef add_action_num_reverse_chrono(df):\n    return df.select([\n        pl.col('*'),\n        pl.col('session').cumcount().reverse().over('session').alias('action_num_reverse_chrono')\n    ])\n\ndef add_session_length(df):\n    return df.select([\n        pl.col('*'),\n        pl.col('session').count().over('session').alias('session_length')\n    ])\n\ndef add_log_recency_score(df):\n    linear_interpolation = 0.1 + ((1-0.1) / (df['session_length']-1)) * (df['session_length']-df['action_num_reverse_chrono']-1)\n    return df.with_columns(pl.Series(2**linear_interpolation - 1).alias('log_recency_score')).fill_nan(1)\n\ndef add_type_weighted_log_recency_score(df):\n    type_weights = {0:1, 1:6, 2:3}\n    type_weighted_log_recency_score = pl.Series(df['log_recency_score'] / df['type'].apply(lambda x: type_weights[x]))\n    return df.with_column(type_weighted_log_recency_score.alias('type_weighted_log_recency_score'))\n\ndef apply(df, pipeline):\n    for f in pipeline:\n        df = f(df)\n    return df\n\npipeline = [add_action_num_reverse_chrono, add_session_length, add_log_recency_score, add_type_weighted_log_recency_score, word2vec_candidate]\n\ntrain = apply(train, pipeline)\n\ngc.collect()\n\ntype2id = {\"clicks\": 0, \"carts\": 1, \"orders\": 2}\n\ntrain_labels = train_labels.explode('ground_truth').with_columns([\n    pl.col('ground_truth').alias('aid'),\n    pl.col('type').apply(lambda x: type2id[x])\n])[['session', 'type', 'aid']]\n\ntrain_labels = train_labels.with_columns([\n    pl.col('session').cast(pl.datatypes.Int32),\n    pl.col('type').cast(pl.datatypes.UInt8),\n    pl.col('aid').cast(pl.datatypes.Int32)\n])\n\ntrain_labels = train_labels.with_column(pl.lit(1).alias('gt'))\n\ntrain = train.join(train_labels, how='left', on=['session', 'type', 'aid']).with_column(pl.col('gt').fill_null(0))\n\ndef get_session_lenghts(df):\n    return df.groupby('session').agg([\n        pl.col('session').count().alias('session_length')\n    ])['session_length'].to_numpy()\n\nsession_lengths_train = get_session_lenghts(train)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T07:38:44.216989Z","iopub.execute_input":"2023-01-03T07:38:44.217673Z","iopub.status.idle":"2023-01-03T07:38:47.493151Z","shell.execute_reply.started":"2023-01-03T07:38:44.217641Z","shell.execute_reply":"2023-01-03T07:38:47.492081Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"start\nw2vec start\nw2vec end\n執行時間: 0.2642664909362793 秒\n\ncandidate calc start\ncandidates\nshape: (2000, 2)\n┌─────────┬─────────┐\n│ session ┆ aid     │\n│ ---     ┆ ---     │\n│ i32     ┆ i32     │\n╞═════════╪═════════╡\n│ 0       ┆ 974651  │\n│ 0       ┆ 543308  │\n│ 0       ┆ 1199474 │\n│ 0       ┆ 1549618 │\n│ ...     ┆ ...     │\n│ 99      ┆ 1262469 │\n│ 99      ┆ 1642686 │\n│ 99      ┆ 82799   │\n│ 99      ┆ 746209  │\n└─────────┴─────────┘\ncandidate calc end\n執行時間: 0.02591562271118164 秒\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let us now define the preprocessing steps we would like to apply to our data.","metadata":{}},{"cell_type":"code","source":"train_ds = Dataset(train.to_pandas())\n\nfeature_cols = ['aid', 'type','action_num_reverse_chrono', 'session_length', 'log_recency_score', 'type_weighted_log_recency_score', 'word2vec_rank']\ntarget = ['gt'] >> AddTags([Tags.TARGET])\nqid_column = ['session'] >>  AddTags([Tags.USER_ID]) # we will use sessions as a query ID column\n                                                     # in XGBoost parlance this a way of grouping together for training\n                                                     # when training with LGBM we had to calculate session lengths, but here the model does all the work for us!\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-01-03T07:38:47.494619Z","iopub.execute_input":"2023-01-03T07:38:47.494940Z","iopub.status.idle":"2023-01-03T07:38:47.694004Z","shell.execute_reply.started":"2023-01-03T07:38:47.494910Z","shell.execute_reply":"2023-01-03T07:38:47.692768Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"175"},"metadata":{}}]},{"cell_type":"markdown","source":"Having defined the preprocessing steps, we can now apply them to our data. The preprocessing is going to run on the GPU!","metadata":{}},{"cell_type":"code","source":"wf = Workflow(feature_cols + target + qid_column)\ntrain_processed = wf.fit_transform(train_ds)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T07:38:47.695894Z","iopub.execute_input":"2023-01-03T07:38:47.696646Z","iopub.status.idle":"2023-01-03T07:38:47.748627Z","shell.execute_reply.started":"2023-01-03T07:38:47.696612Z","shell.execute_reply":"2023-01-03T07:38:47.747782Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"code","source":"ranker = XGBoost(train_processed.schema, objective='rank:pairwise')","metadata":{"execution":{"iopub.status.busy":"2023-01-03T07:38:47.749804Z","iopub.execute_input":"2023-01-03T07:38:47.750154Z","iopub.status.idle":"2023-01-03T07:38:47.756355Z","shell.execute_reply.started":"2023-01-03T07:38:47.750117Z","shell.execute_reply":"2023-01-03T07:38:47.754996Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"The `Distributed` context manager will start a dask cudf cluster of us. A Dask cluster will be able to better manage memory usage for us. Normally, setting it up would be quite tedious -- here, we get all the benefits with a single line of Python code!","metadata":{}},{"cell_type":"code","source":"# version mismatch doesn't result in a loss of functionality here for us\n# it stems from the versions of libraries that the Kaggle vm comes preinstalled with\n\nwith Distributed():\n    ranker.fit(train_processed)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T07:38:47.758405Z","iopub.execute_input":"2023-01-03T07:38:47.759183Z","iopub.status.idle":"2023-01-03T07:38:53.051235Z","shell.execute_reply.started":"2023-01-03T07:38:47.759144Z","shell.execute_reply":"2023-01-03T07:38:53.050182Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/distributed/node.py:161: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 41677 instead\n  f\"Port {expected} is already in use.\\n\"\n","output_type":"stream"},{"name":"stdout","text":"[0]\ttrain-map:1.00000\n[1]\ttrain-map:1.00000\n[2]\ttrain-map:1.00000\n[3]\ttrain-map:1.00000\n[4]\ttrain-map:1.00000\n[5]\ttrain-map:1.00000\n[6]\ttrain-map:1.00000\n[7]\ttrain-map:1.00000\n[8]\ttrain-map:1.00000\n[9]\ttrain-map:1.00000\n","output_type":"stream"},{"name":"stderr","text":"[07:38:52] task [xgboost.dask-1]:tcp://127.0.0.1:35091 got new rank 0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We have now trained our model! Let's predict on test!","metadata":{}},{"cell_type":"markdown","source":"# Predict on test data","metadata":{}},{"cell_type":"markdown","source":"Let's load our test set, process it and predict on it.","metadata":{}},{"cell_type":"code","source":"test = apply(test, pipeline)\ntest_ds = Dataset(test.to_pandas())\n\nwf = wf.remove_inputs(['gt']) # we don't have ground truth information in test!\n\ntest_ds_transformed = wf.transform(test_ds)","metadata":{"execution":{"iopub.status.busy":"2023-01-03T07:38:53.053014Z","iopub.execute_input":"2023-01-03T07:38:53.053322Z","iopub.status.idle":"2023-01-03T07:38:53.117557Z","shell.execute_reply.started":"2023-01-03T07:38:53.053286Z","shell.execute_reply":"2023-01-03T07:38:53.116391Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"candidate calc start\ncandidates\nshape: (2000, 2)\n┌──────────┬─────────┐\n│ session  ┆ aid     │\n│ ---      ┆ ---     │\n│ i32      ┆ i32     │\n╞══════════╪═════════╡\n│ 12899779 ┆ 59625   │\n│ 12899779 ┆ 1219653 │\n│ 12899779 ┆ 977788  │\n│ 12899779 ┆ 646708  │\n│ ...      ┆ ...     │\n│ 12899878 ┆ 1597140 │\n│ 12899878 ┆ 1040641 │\n│ 12899878 ┆ 1689148 │\n│ 12899878 ┆ 372062  │\n└──────────┴─────────┘\ncandidate calc end\n執行時間: 0.010401248931884766 秒\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's output the predictions","metadata":{}},{"cell_type":"code","source":"test_preds = ranker.booster.predict(xgb.DMatrix(test_ds_transformed.compute()))","metadata":{"execution":{"iopub.status.busy":"2023-01-03T07:38:53.119043Z","iopub.execute_input":"2023-01-03T07:38:53.119345Z","iopub.status.idle":"2023-01-03T07:38:53.148776Z","shell.execute_reply.started":"2023-01-03T07:38:53.119319Z","shell.execute_reply":"2023-01-03T07:38:53.147876Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Create submission","metadata":{}},{"cell_type":"code","source":"test = test.with_columns(pl.Series(name='score', values=test_preds))\ntest_predictions = test.sort(['session', 'score'], reverse=True).groupby('session').agg([\n    pl.col('aid').limit(20).list()\n])","metadata":{"execution":{"iopub.status.busy":"2023-01-03T07:38:53.153005Z","iopub.execute_input":"2023-01-03T07:38:53.153324Z","iopub.status.idle":"2023-01-03T07:38:53.162210Z","shell.execute_reply.started":"2023-01-03T07:38:53.153294Z","shell.execute_reply":"2023-01-03T07:38:53.160763Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"session_types = []\nlabels = []\n\nfor session, preds in zip(test_predictions['session'].to_numpy(), test_predictions['aid'].to_numpy()):\n    l = ' '.join(str(p) for p in preds)\n    for session_type in ['clicks', 'carts', 'orders']:\n        labels.append(l)\n        session_types.append(f'{session}_{session_type}')","metadata":{"execution":{"iopub.status.busy":"2023-01-03T07:38:53.163828Z","iopub.execute_input":"2023-01-03T07:38:53.164532Z","iopub.status.idle":"2023-01-03T07:38:53.175142Z","shell.execute_reply.started":"2023-01-03T07:38:53.164495Z","shell.execute_reply":"2023-01-03T07:38:53.173788Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"submission = pl.DataFrame({'session_type': session_types, 'labels': labels})\nsubmission.write_csv('xgb_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-03T07:38:53.176620Z","iopub.execute_input":"2023-01-03T07:38:53.177163Z","iopub.status.idle":"2023-01-03T07:38:53.183889Z","shell.execute_reply.started":"2023-01-03T07:38:53.177131Z","shell.execute_reply":"2023-01-03T07:38:53.182818Z"},"trusted":true},"execution_count":37,"outputs":[]}]}